{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/DS3001/wrangling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NOvcayM6dwu",
        "outputId": "4cb01853-e8cf-42a9-a10c-b26f3c55707d"
      },
      "id": "1NOvcayM6dwu",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wrangling'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 63 (delta 22), reused 27 (delta 10), pack-reused 21\u001b[K\n",
            "Receiving objects: 100% (63/63), 6.75 MiB | 12.68 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bXWtc-pyuugf"
      },
      "id": "bXWtc-pyuugf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tidy Data talks about the efforts taken when cleaning data and a techique used to clean data easier. It takes about the benefits of usig this structure.\n",
        "2. The \"tidy data standard\" intends to make data cleaning easier along with simplifying development of data analysis.\n",
        "3. Sentence 1 tells us that tidy datasets are easy to digest but messy datsets are not. Sentence 2 says it difficult to understand data unless they are in a dataset or sorted.\n",
        "4.  Wickham states that values are organized in a variable and an observation.\n",
        "5. In section 2.3, Tidy Data is defined as a standard way of mapping the meaning of a dataset to its structure.\n",
        "6. Issues mentioend include: column headers, multiple variables in one column, variables in rows and columns, multiple observational units in the same table, and single obervational unit stored in multiple tables. Table 4 is messy because there is too much happening in the chart and is difficult to read because of the categories. Melting a dataset is turning columns into rows.\n",
        "7. Table 11 is messy because there are many gaps within the chart and also does not organize the dates well while Table 12 is more specific when organizing the dates in the molten data the elements column contains names of variables. Tidy data fixes the issues with the columns an dhas numbers that represent the variables and numbers given.\n",
        "8. It is when tody data is only useful to tools what work with it then it will be linked to tidy data. He hopes others will build on tiday framework to develop even better data storage strategies."
      ],
      "metadata": {
        "id": "7Xg6qi1-0gQs"
      },
      "id": "7Xg6qi1-0gQs"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5ws7U8bLNr__"
      },
      "id": "5ws7U8bLNr__"
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2.1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "data = pd.read_csv('./wrangling/assignment/data/airbnb_hw.csv', low_memory= False)\n",
        "#url = #example of url\n",
        "#data = pd.read_csv(url,low_memory = False)\n",
        "print(data.shape,'\\n')\n",
        "data.head()\n",
        "price = data[\"Price\"]\n",
        "price = price.str.replace(',', '') # to remove commas\n",
        "print('Missing:', sum(price.isnull())) # shows all missing values\n",
        "print(price.unique())\n",
        "# end up with 0 mizzing values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXSLZtHn0aJH",
        "outputId": "5a6da993-4547-4294-9d71-ad0b02381f12"
      },
      "id": "EXSLZtHn0aJH",
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(30478, 13) \n",
            "\n",
            "Missing: 0\n",
            "['145' '37' '28' '199' '549' '149' '250' '90' '270' '290' '170' '59' '49'\n",
            " '68' '285' '75' '100' '150' '700' '125' '175' '40' '89' '95' '99' '499'\n",
            " '120' '79' '110' '180' '143' '230' '350' '135' '85' '60' '70' '55' '44'\n",
            " '200' '165' '115' '74' '84' '129' '50' '185' '80' '190' '140' '45' '65'\n",
            " '225' '600' '109' '1990' '73' '240' '72' '105' '155' '160' '42' '132'\n",
            " '117' '295' '280' '159' '107' '69' '239' '220' '399' '130' '375' '585'\n",
            " '275' '139' '260' '35' '133' '300' '289' '179' '98' '195' '29' '27' '39'\n",
            " '249' '192' '142' '169' '1000' '131' '138' '113' '122' '329' '101' '475'\n",
            " '238' '272' '308' '126' '235' '315' '248' '128' '56' '207' '450' '215'\n",
            " '210' '385' '445' '136' '247' '118' '77' '76' '92' '198' '205' '299'\n",
            " '222' '245' '104' '153' '349' '114' '320' '292' '226' '420' '500' '325'\n",
            " '307' '78' '265' '108' '123' '189' '32' '58' '86' '219' '800' '335' '63'\n",
            " '229' '425' '67' '87' '1200' '158' '650' '234' '310' '695' '400' '166'\n",
            " '119' '62' '168' '340' '479' '43' '395' '144' '52' '47' '529' '187' '209'\n",
            " '233' '82' '269' '163' '172' '305' '156' '550' '435' '137' '124' '48'\n",
            " '279' '330' '5000' '134' '378' '97' '277' '64' '193' '147' '186' '264'\n",
            " '30' '3000' '112' '94' '379' '57' '415' '236' '410' '214' '88' '66' '71'\n",
            " '171' '157' '545' '1500' '83' '96' '1800' '81' '188' '380' '255' '505'\n",
            " '54' '33' '174' '93' '740' '640' '1300' '440' '599' '357' '1239' '495'\n",
            " '127' '5999' '178' '348' '152' '242' '183' '253' '750' '259' '365' '273'\n",
            " '197' '397' '103' '389' '355' '559' '38' '203' '999' '141' '162' '333'\n",
            " '698' '46' '360' '895' '10' '41' '206' '281' '449' '388' '212' '102'\n",
            " '201' '2750' '4750' '432' '675' '167' '390' '298' '339' '194' '302' '211'\n",
            " '595' '191' '53' '361' '480' '8000' '4500' '459' '997' '345' '216' '218'\n",
            " '111' '735' '276' '91' '490' '850' '398' '36' '775' '267' '625' '336'\n",
            " '2500' '176' '725' '3750' '469' '106' '460' '287' '575' '227' '263' '25'\n",
            " '228' '208' '177' '880' '148' '116' '685' '470' '217' '164' '61' '645'\n",
            " '699' '405' '252' '319' '268' '419' '343' '525' '311' '840' '154' '294'\n",
            " '950' '409' '184' '257' '204' '241' '2000' '412' '121' '288' '196' '900'\n",
            " '647' '524' '1750' '309' '510' '1495' '1700' '799' '383' '372' '492'\n",
            " '327' '1999' '656' '224' '173' '875' '1170' '795' '690' '146' '465'\n",
            " '1100' '151' '274' '429' '825' '282' '256' '1111' '620' '271' '161' '51'\n",
            " '855' '579' '1174' '430' '20' '899' '649' '485' '181' '455' '4000' '243'\n",
            " '342' '590' '560' '374' '437' '232' '359' '985' '31' '244' '254' '723'\n",
            " '237' '428' '370' '34' '1400' '580' '2520' '221' '749' '1600' '2695'\n",
            " '306' '202' '680' '570' '520' '223' '2295' '213' '1065' '346' '24' '286'\n",
            " '296' '266' '26' '995' '1368' '393' '182' '635' '258' '780' '589' '347'\n",
            " '1250' '1350' '446' '3200' '1050' '1650' '1550' '975' '323' '6500' '2499'\n",
            " '1850' '2250' '715' '461' '540' '356' '439' '384' '569' '1900' '22' '785'\n",
            " '626' '830' '318' '444' '321' '401' '1499' '888' '369' '770' '386' '366'\n",
            " '344' '630' '313' '597' '262' '509' '10000' '278' '312' '789' '1195'\n",
            " '422' '21' '765' '3500' '945' '326' '3100' '2486' '3390' '1356' '2599'\n",
            " '472' '454' '328' '396' '291']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2.2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "data = pd.read_csv('./wrangling/assignment/data/sharks.csv', low_memory= False)\n",
        "#print(data)\n",
        "data['Type'].value_counts() #looks at each 'type' and views how many values each contains\n",
        "type = data['Type']\n",
        "type = type.replace(['Sea Disaster','Boat','Boatomg','Boating'],'Watercraft')\n",
        "type.value_counts() #shows new watercrafts into one\n",
        "\n",
        "#organizing the random variables\n",
        "type = type.replace(['Questionable','Unconfirmed','Invalid','Under investigation'],'Unverified')\n",
        "type.value_counts()\n",
        "\n",
        "data['Type'] = type #replacing the old to tidy data\n",
        "del type\n",
        "\n",
        "data['Type'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB3z1WTiuTcV",
        "outputId": "4ddae69f-5237-4209-a935-e32769cbed86"
      },
      "id": "BB3z1WTiuTcV",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unprovoked    4716\n",
              "Provoked       593\n",
              "Watercraft     583\n",
              "Unverified     565\n",
              "Name: Type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race? [based on self-identification and get it from federal state, local governemnts, and come commerical entities]\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter? [Race data are used in planning and funding government programs that provide funds or services for specific groups.]\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data? [In order to eliminate any biases it would be good to collect data from schools or where it is required for many people to input their race. I think that gathering it from the federal state is good to represent each state.]\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.[The U.S. Census Bureau's gender-identity test asks respondents about their current gender and the sex they were assigned at birth. It could change or be inaccurate for people who identify as intersex.]\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?[Some might not be comfortable with sharing data regarding their sex or race and there is an option which will not represent a small percentage of the population.]\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have? [It could be unethicial or inaccurate. ]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}